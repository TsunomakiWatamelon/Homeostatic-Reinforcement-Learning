{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience : Surconsommation de Nourriture Hyperpalatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of environments.hyperpalatable_env failed: Traceback (most recent call last):\n",
      "  File \"/home/hocine/miniconda3/envs/deepdac/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/hocine/miniconda3/envs/deepdac/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/hocine/miniconda3/envs/deepdac/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/hocine/miniconda3/envs/deepdac/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/hocine/miniconda3/envs/deepdac/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/hocine/miniconda3/envs/deepdac/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 0 free vars, not 1\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Ajouter le dossier src au chemin d'importation\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.hyperpalatable_env import HyperpalatableEnvironment\n",
    "from models.qlearning import QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = torch.tensor([10])  # État interne initial\n",
    "setpoints = torch.tensor([20])  # Setpoint pour l'état optimal\n",
    "weights = torch.tensor([1.0])  # Poids de l'état\n",
    "exponents = [2, 2]  # Non-linéarité du drive\n",
    "effects = {0: torch.tensor([1]), 1: torch.tensor([2])}  # Effets des actions\n",
    "palatability_bonus = 5  # Bonus pour nourriture hyperpalatable\n",
    "normal_food_reward = 2  # Récompense pour nourriture normale\n",
    "\n",
    "# Instanciation de l'environnement\n",
    "env = HyperpalatableEnvironment(\n",
    "    H=initial_state,\n",
    "    setpoints=setpoints,\n",
    "    weights=weights,\n",
    "    exponents=exponents,\n",
    "    effects=effects,\n",
    "    palatability_bonus=palatability_bonus,\n",
    "    normal_food_reward=normal_food_reward,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration de l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la Q-table\n",
    "state_size = 21  # Taille pour inclure les états de 0 à 20\n",
    "action_size = 2  # Deux actions possibles : nourriture normale ou hyperpalatable\n",
    "q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "# Configuration de l'agent Q-learning\n",
    "alpha = 0.1  # Taux d'apprentissage\n",
    "gamma = 0.96  # Facteur de discount\n",
    "epsilon = 0.1  # Facteur d'exploration\n",
    "agent = QLearning(state_size, action_size, alpha, gamma, epsilon, q_table=q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"L'état 10 n'existe pas dans la table Q.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Choisir une action\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Effectuer une étape\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/github/Homeostatic-Reinforcement-Learning/src/models/qlearning.py:29\u001b[0m, in \u001b[0;36mQLearning.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Vérification de l'existence de l'état dans la table Q\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124métat \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexiste pas dans la table Q.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Vérification que des actions sont disponibles\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[state]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: \"L'état 10 n'existe pas dans la table Q.\""
     ]
    }
   ],
   "source": [
    "# Hyperparamètres d'apprentissage\n",
    "episodes = 5000  # Nombre total d'épisodes\n",
    "reward_history = []  # Historique des récompenses par épisode\n",
    "\n",
    "# Boucle d'apprentissage\n",
    "for episode in range(episodes):\n",
    "    state = env.state.clone().item()  # État initial\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choisir une action\n",
    "        action = agent.choose_action(int(state))\n",
    "\n",
    "        # Effectuer une étape\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Mise à jour Q-learning\n",
    "        agent.update_q_value(int(state), action, reward, int(next_state.item()))\n",
    "\n",
    "        # Mise à jour de l'état courant\n",
    "        state = next_state.item()\n",
    "        total_reward += reward\n",
    "\n",
    "    reward_history.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des résultats\n",
    "plt.plot(reward_history)\n",
    "plt.title(\"Historique des récompenses\")\n",
    "plt.xlabel(\"Épisodes\")\n",
    "plt.ylabel(\"Récompense cumulée\")\n",
    "plt.show()\n",
    "\n",
    "# Politique optimale\n",
    "optimal_policy = [np.argmax(agent.q_table[state]) for state in range(state_size)]\n",
    "print(\"Politique optimale :\", optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
